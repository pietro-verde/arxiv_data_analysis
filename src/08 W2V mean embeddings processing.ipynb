{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d255a1f-da54-4a69-872b-04d349b0587e",
   "metadata": {},
   "source": [
    "## Create google word2vec mean vectors for Naive Bayes and Logistic Regression\n",
    "\n",
    "\n",
    "Word embeddings are ways to calculate vectors that represent words, through a system where words that have similar meaning also have similar vectors which fall close to each other in the vector space. The dimension of the vectors can vary and so do the methods to compute them. Word2Vec is one of the most successful word embeddings techniques and it proposes two main algorithms for efficiently learning vector representations of words from large corpus of text data: CBOW (Continuous Bag of Words) and Skip-gram. Both algorithms are neural network based.\n",
    "\n",
    "One of the main problems that word embeddings solve, along with the ability to capture semantic, is that of sparsity. In fact, unlike BOW and TF-IFD, these techniques produce dense, low-dimensional vectors, which are a big step forward from having to use a matrix that could have had just as many columns as the total number of words contained in the entire vocabulary.\n",
    "\n",
    "The version of Word2Vec embeddings used here are 300-dimensional vectors representing 3 million words and phrases and were pre-trained on 100 billion words from the [Google News dataset](https://code.google.com/archive/p/word2vec/).\n",
    "\n",
    "In order to use work embeddings with the Na√Øve Bayes and Logistic Regression algorithms, the words present in each document are looked up from the set of embeddings available and all the vectors obtained for each word are normalized and then averaged. The result is a 300 dimension mean vector for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b085212d-977e-4d5d-8357-3771bf5fffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as gensim_api\n",
    "import pandas as pd\n",
    "import swifter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d956cdfb-f6e6-4e7f-8ae0-7ddc7758441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"../data/data.parquet.gzip\"\n",
    "data = pd.read_parquet(FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8415ed94-e38d-4aae-befb-e3b19ff9c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim_api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d1246ab-abc0-4871-b7fb-fff13b827618",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenght = len(data)\n",
    "vecs = []\n",
    "for i, txt in enumerate(data[\"processed_docs\"]):\n",
    "    words = txt.split()\n",
    "    vec = wv.get_mean_vector(words,pre_normalize=True)\n",
    "    vecs.append(vec)\n",
    "    p = round(i/lenght,4)*100\n",
    "    print(f'{i} of {lenght} - {p} % ', end='\\r')\n",
    "\n",
    "save_as=\"vecs_prenorm\"\n",
    "pickle.dump(vecs, open(\"tmp/\"+save_as+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04195890-1479-4429-b8c9-4c8906d850f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = pickle.load(open(\"tmp/\"+\"vecs_prenorm.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cf6f999-a35a-4902-a536-ebfc657117bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"google-news_w2v_mean_prenorm\"] = vecs\n",
    "\n",
    "data[[\"id\",'target','google-news_w2v_mean_prenorm']].to_parquet(\"../data/w2v-vectors.parquet.gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
